{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "U4Eusl4OQS3z",
        "k-NWyfI3Qek8",
        "7toDhL0Lxyoe",
        "NjhPE_MuRtbB",
        "A5-Oho1oAVsC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devanshu1204/Exp_Agentic-RAG/blob/main/Copy_of_Working_Agentic_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7tBRwztW-xe"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv==1.0.0\n",
        "!pip install llama-index==0.10.27\n",
        "!pip install llama-index-readers-file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "OgL-Ra4mb5t7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "7DQnEanZQ6Wh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from typing import List, Optional\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "9z_nRPZSYyNk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking and creating Vector Embeddings"
      ],
      "metadata": {
        "id": "OKONlIP8fQE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load documents\n",
        "documents = SimpleDirectoryReader(input_files=[\n",
        "    \"A_mon.pdf\",\n",
        "    \"D_mon.pdf\"\n",
        "]).load_data()\n",
        "\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# Check if nodes are created successfully\n",
        "if nodes:\n",
        "        print(\"Nodes created successfully.\")\n",
        "        print(f\"Total nodes created: {len(nodes)}\")\n",
        "\n",
        "        # Print some nodes for debugging\n",
        "        print(\"Printing first 5 nodes:\")\n",
        "        for i, node in enumerate(nodes[:5]):\n",
        "            print(f\"Node {i+1}: {node}\")\n",
        "else:\n",
        "        print(\"Failed to create nodes.\")\n",
        "\n",
        "\n",
        "print(\"starting vector indexing by creating embeddings using ada 002\")\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "print(\"finished vector indexing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdRsv_srZK2m",
        "outputId": "5b38fa5f-8003-4dd3-fe91-5fea366459a8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes created successfully.\n",
            "Total nodes created: 2\n",
            "Printing first 5 nodes:\n",
            "Node 1: Node ID: 61ceb9be-b6dd-4ccb-bc96-84507a1de1a2\n",
            "Text: Anukalp’s monthly income is 25 thousand US $\n",
            "Node 2: Node ID: 359df5e5-ee42-454e-9e6a-44087972464d\n",
            "Text: Devanshu’s monthly income is 50 thousand US $\n",
            "starting vector indexing by creating embeddings using ada 002\n",
            "finished vector indexing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing vector Embeddings"
      ],
      "metadata": {
        "id": "U4Eusl4OQS3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_index.storage_context.persist()"
      ],
      "metadata": {
        "id": "3DB5hjdPFerP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "index = load_index_from_storage(storage_context=storage_context)"
      ],
      "metadata": {
        "id": "ydQQIZE5Pr2y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for Creating Tools"
      ],
      "metadata": {
        "id": "k-NWyfI3Qek8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "\n",
        "    #print(\"starting embeddings\")\n",
        "\n",
        "    # Create embeddings for the nodes using the sentence transformer model\n",
        "    #embeddings = embedding_model.encode(tokenized_texts)\n",
        "    #for i, node in enumerate(nodes):\n",
        "    #    node.embedding = embeddings[i].tolist()\n",
        "\n",
        "    #for i, node in enumerate(nodes):\n",
        "     #print(f\"Node {i+1} embedding:\")\n",
        "     #print(node.embedding)\n",
        "\n",
        "\n",
        "    #print(\"finished embeddings\")\n",
        "\n",
        "\n",
        "\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Use to answer questions over a given paper.\n",
        "\n",
        "        Useful if you have specific questions over the paper.\n",
        "        Always leave page_numbers as None UNLESS there is a specific page you want to search for.\n",
        "\n",
        "        Args:\n",
        "            query (str): the string query to be embedded.\n",
        "            page_numbers (Optional[List[str]]): Filter by set of pages. Leave as NONE\n",
        "                if we want to perform a vector search\n",
        "                over all pages. Otherwise, filter by the set of specified pages.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR\n",
        "            )\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "\n",
        "\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ],
      "metadata": {
        "id": "CLp1yb8ffPmR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calling the Function for creating Tools"
      ],
      "metadata": {
        "id": "7toDhL0Lxyoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "papers = [\n",
        "    \"A_mon.pdf\",\n",
        "    \"D_mon.pdf\"\n",
        "]\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, paper.replace(\".pdf\", \"\"))\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n"
      ],
      "metadata": {
        "id": "fqmf1hcaHajt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ],
      "metadata": {
        "id": "M2GPW4O0zuH5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(initial_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc09hkAWUxGd",
        "outputId": "d4ce64e1-e516-4406-8347-f27185c83e14"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<llama_index.core.tools.function_tool.FunctionTool object at 0x7c95d7d3f4c0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7c95d4f24d00>, <llama_index.core.tools.function_tool.FunctionTool object at 0x7c95d4f265f0>, <llama_index.core.tools.query_engine.QueryEngineTool object at 0x7c95d4f25bd0>]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7--ekoKo3XDw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(initial_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfABekwBV3Yq",
        "outputId": "f4eb5bb8-7931-4afc-a2a4-8d3053a42e9a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "Q9sOLBBS3qPR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA"
      ],
      "metadata": {
        "id": "NjhPE_MuRtbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Who earns more salary\"\n",
        ")"
      ],
      "metadata": {
        "id": "b3nCgF-e3q58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.query(\n",
        "    \"Who earns 600,000 yearly\"\n",
        ")"
      ],
      "metadata": {
        "id": "AyEJVIxN6rhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings are being stored here if we directly use vectorStoreIndex since it handles the embeddings internally and stores them in memeory"
      ],
      "metadata": {
        "id": "A5-Oho1oAVsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if hasattr(vector_index, 'vector_store'):\n",
        "        print(\"Vector index vector_store attributes:\")\n",
        "        print(vector_index.vector_store.__dict__)"
      ],
      "metadata": {
        "id": "JbSSuTqEAv3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}